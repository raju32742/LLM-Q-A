{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "!pip install nltk\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install tokenizers\n",
    "!pip install evaluate\n",
    "!pip install rouge-score\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset,Dataset, DatasetDict\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Replace 'your_api_key' with your actual API key\n",
    "wandb.login(key=\"42ec63b91e907bed87b6dc91680e063c2c5cbe27\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MedQuAD.csv')  # Replace with your dataset path\n",
    "df = dataset\n",
    "dataset = dataset.drop('qtype', axis=1)\n",
    "dataset = dataset.rename(columns={'Question': 'question', 'Answer': 'answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Check the unique values in the 'qtype' column\n",
    "unique_qtypes = df['qtype'].unique()\n",
    "\n",
    "# Display the distribution of question types\n",
    "qtype_distribution = df['qtype'].value_counts()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "qtype_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Question Types')\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Display the unique question types\n",
    "print(\"Unique Question Types:\", unique_qtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer_Length_Words'] = df['Answer'].str.split().apply(len)\n",
    "# Visualize the distribution of answer lengths\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df['Answer_Length_Words'], bins=100, color='salmon', edgecolor='black')\n",
    "plt.title('Answer Length Distribution (Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lowercasing the text.\n",
    "* Removing punctuation.\n",
    "* Removing stopwords.\n",
    "* Removing frequent words.\n",
    "* Removing rare words.\n",
    "* Removing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Download the necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = dataset\n",
    "# Lowercasing\n",
    "# Lowercasing\n",
    "df['question'] = df['question'].str.lower()\n",
    "df['answer'] = df['answer'].str.lower()\n",
    "# Remove punctuation\n",
    "df['question'] = df['question'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df['answer'] = df['answer'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of question words to retain\n",
    "question_words = {'who', 'what', 'where', 'when', 'why', 'how', 'is', 'are'}\n",
    "\n",
    "# Define stopwords excluding question words\n",
    "stop_words = set(stopwords.words('english')) - question_words\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['question'] = df['question'].apply(remove_stopwords)\n",
    "df['answer'] = df['answer'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "# Lemmatization\n",
    "# Initialize stemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()  # Alternative: more aggressive stemming\n",
    "\n",
    "# Function to stem text\n",
    "def stem_text(text, stemmer):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "# Apply stemming\n",
    "df['question'] = df['question'].apply(lambda x: stem_text(x, porter))\n",
    "df['answer'] = df['answer'].apply(lambda x: stem_text(x, porter))\n",
    "\n",
    "# Display processed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = df\n",
    "df_full_train, df_test = train_test_split(dataset, test_size=0.2, random_state=56)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict['validation']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# Set the model name\n",
    "MODEL_NAME = 'google/flan-t5-base' \n",
    "# Load the model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocess function\n",
    "def preprocess_function(data):\n",
    "    inputs = [q for q in data[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=64, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(data[\"answer\"], max_length=200, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "tokenized_datasets = health_dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_datasets['validation']['question'][1])\n",
    "print(tokenized_datasets['train']['question'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"MEdQuAD/results/t5\"  # Replace with a directory where you have write permissions\n",
    "  # Replace with a directory where you have write permissions\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    logging_dir=os.path.join(output_dir, 'logs'),  # Set directory for logs\n",
    "    logging_strategy=\"steps\",  # Log at each step\n",
    "    logging_steps=10,          # Number of steps between logging\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.001,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=50,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(\"/kaggle/working/results/t5/trained_model\")  # Change this to your desired directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Function to generate predictions\n",
    "def generate_predictions(dataset, model, tokenizer, device, batch_size=16):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = tokenizer(batch['question'], return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=200)\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        refs = batch['answer']\n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "# Generate predictions for the test dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predictions, references = generate_predictions(tokenized_datasets['test'], model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Initialize metrics\n",
    "if len(predictions) == 0 or len(references) == 0:\n",
    "    print(\"No predictions or references to evaluate.\")\n",
    "else:\n",
    "    # Initialize metrics\n",
    "    smooth = SmoothingFunction().method4\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    bleu1_scores = []\n",
    "    bleu4_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        # BLEU-1 and BLEU-4\n",
    "        bleu1 = sentence_bleu([ref.split()], pred.split(), weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "        bleu4 = sentence_bleu([ref.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "        # ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "        rouge_scores = rouge.score(ref, pred)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "    # Check if there are any scores to average\n",
    "    if len(bleu1_scores) == 0:\n",
    "        avg_bleu1 = avg_bleu4 = avg_rouge1 = avg_rouge2 = avg_rougeL = 0\n",
    "    else:\n",
    "        # Average the scores\n",
    "        avg_bleu1 = sum(bleu1_scores) / len(bleu1_scores)\n",
    "        avg_bleu4 = sum(bleu4_scores) / len(bleu4_scores)\n",
    "        avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "        avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "        avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    # Print the results\n",
    "    print(f\"BLEU-1 Score: {avg_bleu1}\")\n",
    "    print(f\"BLEU-4 Score: {avg_bleu4}\")\n",
    "    print(f\"ROUGE-1 Score: {avg_rouge1}\")\n",
    "    print(f\"ROUGE-2 Score: {avg_rouge2}\")\n",
    "    print(f\"ROUGE-L Score: {avg_rougeL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Tokenize the sentences (split by space for simplicity, but consider using more sophisticated tokenization if needed)\n",
    "tokenized_references = [ref.split() for ref in references]\n",
    "tokenized_hypotheses = [hyp.split() for hyp in predictions]\n",
    "# Calculate METEOR scores for each reference-hypothesis pair\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(tokenized_references, tokenized_hypotheses)]\n",
    "\n",
    "# Average the scores (if multiple pairs are present)\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "# Print the results\n",
    "print(f\"METEOR Score: {avg_meteor}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4054084,
     "sourceId": 7045374,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
