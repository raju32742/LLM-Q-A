{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b07a8e-6b6b-4927-9549-3babe9c48383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering,GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc73c0-a9d0-485f-9f59-4202088f8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eeed63-2e3a-40c2-940e-ded80bb6f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Replace 'your_api_key' with your actual API key\n",
    "wandb.login(key=\"42ec63b91e907bed87b6dc91680e063c2c5cbe27\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fd883-35ab-4155-ba0d-cc9cdcd9f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "dataset = pd.read_csv('MedQuAD.csv')  # Replace with your dataset path\n",
    "df = dataset\n",
    "dataset = dataset.drop('qtype', axis=1)\n",
    "dataset = dataset.rename(columns={'Question': 'question', 'Answer': 'answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d53aac-9adc-4b21-b916-cd4980a7a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Check the unique values in the 'qtype' column\n",
    "unique_qtypes = df['qtype'].unique()\n",
    "\n",
    "# Display the distribution of question types\n",
    "qtype_distribution = df['qtype'].value_counts()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "qtype_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Question Types')\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Display the unique question types\n",
    "print(\"Unique Question Types:\", unique_qtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562692e-131f-44d3-b7e4-186af077965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer_Length_Words'] = df['Answer'].str.split().apply(len)\n",
    "# Visualize the distribution of answer lengths\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df['Answer_Length_Words'], bins=100, color='salmon', edgecolor='black')\n",
    "plt.title('Answer Length Distribution (Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f15fd4-b383-4fae-8d2f-25fc98b7aa3c",
   "metadata": {},
   "source": [
    "* Lowercasing the text.\n",
    "* Removing punctuation.\n",
    "* Removing stopwords.\n",
    "* Removing frequent words.\n",
    "* Removing rare words.\n",
    "* Removing emoticons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6c1d7-c1f6-4779-bbe2-5fa993dc724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Download the necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = dataset\n",
    "# Lowercasing\n",
    "# Lowercasing\n",
    "df['question'] = df['question'].str.lower()\n",
    "df['answer'] = df['answer'].str.lower()\n",
    "# Remove punctuation\n",
    "df['question'] = df['question'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df['answer'] = df['answer'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed1d3b-8dfa-44f8-8084-a622dbd85193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of question words to retain\n",
    "question_words = {'who', 'what', 'where', 'when', 'why', 'how'}\n",
    "\n",
    "# Define stopwords excluding question words\n",
    "stop_words = set(stopwords.words('english')) - question_words\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['question'] = df['question'].apply(remove_stopwords)\n",
    "df['answer'] = df['answer'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0b9cc-1ecb-4571-8639-41b0a13b3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "# Lemmatization\n",
    "# Initialize stemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()  # Alternative: more aggressive stemming\n",
    "\n",
    "# Function to stem text\n",
    "def stem_text(text, stemmer):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "# Apply stemming\n",
    "df['question'] = df['question'].apply(lambda x: stem_text(x, porter))\n",
    "df['answer'] = df['answer'].apply(lambda x: stem_text(x, porter))\n",
    "\n",
    "# Display processed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655ef51-bda3-416e-965e-a7e6855692f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df\n",
    "df_full_train, df_test = train_test_split(dataset, test_size=0.2, random_state=56)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=56)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "health_dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61fe5e4-6476-40c4-b788-239994eee889",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe72da-4b3a-490b-a00f-8b481b7dfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d57bb-bea4-454d-a0f7-cd16db4b1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49df24d-472f-482a-b0a6-00b6f53fcf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    inputs = [q + \" [SEP] \" + a for q, a in zip(data[\"question\"], data[\"answer\"])]\n",
    "   # The \"inputs\" are the tokenized answer:\n",
    "#    inputs = [doc for doc in examples[\"question\"] + \" [SEP] \" + doc for doc in examples[\"answer\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=200, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "  \n",
    "   # The \"labels\" are the tokenized outputs:\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d09cb-880e-4b10-bbb8-a808864e90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = health_dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb745e-4e7d-41b7-9870-4bd8909a377a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02838b0a-007c-4e77-918d-a4f17398132d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a98283-fedd-45e0-83fb-095bed80c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a writable directory for outputs\n",
    "import os\n",
    "output_dir = \"MEdQuAD/results/gpt2\"  # Replace with a directory where you have write permissions\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49fdc5-0b2a-4e65-a13f-f7451980088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc7fb4-2a4f-405f-8dc7-601f0b740596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    logging_dir=os.path.join(output_dir, 'logs'),  # Set directory for logs\n",
    "    logging_strategy=\"steps\",  # Log at each step\n",
    "    logging_steps=10,          # Number of steps between logging\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.001,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=50,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8829b-89f0-47a7-9fe7-00c7fcb92fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(MEdQuAD/results/gpt2/trained_model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a454815-7deb-4a22-8487-5652d637a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract logs from the trainer\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Initialize dictionaries to store epoch-wise losses\n",
    "train_loss_by_epoch = {}\n",
    "eval_loss_by_epoch = {}\n",
    "\n",
    "# Iterate through the logs to collect train and eval losses\n",
    "for log in log_history:\n",
    "    if 'loss' in log and 'epoch' in log:\n",
    "        epoch = int(log['epoch'])\n",
    "        train_loss_by_epoch[epoch] = log['loss']\n",
    "    if 'eval_loss' in log and 'epoch' in log:\n",
    "        epoch = int(log['epoch'])\n",
    "        eval_loss_by_epoch[epoch] = log['eval_loss']\n",
    "\n",
    "# Sort epochs and align train and eval losses\n",
    "sorted_epochs = sorted(set(train_loss_by_epoch.keys()).union(set(eval_loss_by_epoch.keys())))\n",
    "train_losses = [train_loss_by_epoch.get(epoch, None) for epoch in sorted_epochs]\n",
    "eval_losses = [eval_loss_by_epoch.get(epoch, None) for epoch in sorted_epochs]\n",
    "\n",
    "# Plotting the loss curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sorted_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(sorted_epochs, eval_losses, label='Evaluation Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ddc88-2afd-4c6d-9a1f-19562a49b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2437726-8426-43c0-b883-5f974353c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "last_checkpoint = \"MEdQuAD/results/gpt2/checkpoint-123050\"\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(last_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "# Set the model to evaluation mode\n",
    "finetuned_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c84c6-0b02-4da5-a868-c21d23d85b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"Who is at risk for Lymphocytic Choriomeningitis (LCM)?\"\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0021c19-e270-4a9b-90ea-37b21a758693",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define a function to generate answers\n",
    "def generate_answer(question, model, tokenizer):\n",
    "    inputs = tokenizer(question, return_tensors='pt', max_length=200, truncation=True, padding=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the device\n",
    "    # Set attention mask\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'], \n",
    "            attention_mask=attention_mask,  # Use attention mask\n",
    "            max_length=200, \n",
    "            num_beams=1, \n",
    "            early_stopping=True, \n",
    "            pad_token_id=tokenizer.eos_token_id  # Set pad token ID\n",
    "        )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sep_token = \"[SEP]\"\n",
    "    if sep_token in answer:\n",
    "        question_part = answer.split(sep_token)[0].strip()\n",
    "        answer = answer[len(question_part):].strip()\n",
    "        # Also remove [SEP] and any leading punctuation or whitespace\n",
    "        answer = answer.lstrip(sep_token + \" ,.\")\n",
    "    return answer\n",
    "\n",
    "# Initialize lists for predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# Iterate over the test dataset and generate predictions\n",
    "for data in tokenized_dataset['test']:\n",
    "    question = data[\"question\"]\n",
    "    reference = data[\"answer\"]\n",
    "    predicted_answer = generate_answer(question, model, tokenizer)\n",
    "    predictions.append(predicted_answer)\n",
    "    references.append(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d1825-aa29-47b6-815b-75463eea7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['test']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94170f4-1de9-4d91-b5fa-d4db33f9d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c5cc7-6063-4fe7-9799-655d501837ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363d1f2-cca5-42af-95ba-9e083e2f9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Initialize metrics\n",
    "if len(predictions) == 0 or len(references) == 0:\n",
    "    print(\"No predictions or references to evaluate.\")\n",
    "else:\n",
    "    # Initialize metrics\n",
    "    smooth = SmoothingFunction().method4\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    bleu1_scores = []\n",
    "    bleu4_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        # BLEU-1 and BLEU-4\n",
    "        bleu1 = sentence_bleu([ref.split()], pred.split(), weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "        bleu4 = sentence_bleu([ref.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "        # ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "        rouge_scores = rouge.score(ref, pred)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "    # Check if there are any scores to average\n",
    "    if len(bleu1_scores) == 0:\n",
    "        avg_bleu1 = avg_bleu4 = avg_rouge1 = avg_rouge2 = avg_rougeL = 0\n",
    "    else:\n",
    "        # Average the scores\n",
    "        avg_bleu1 = sum(bleu1_scores) / len(bleu1_scores)\n",
    "        avg_bleu4 = sum(bleu4_scores) / len(bleu4_scores)\n",
    "        avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "        avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "        avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    # Print the results\n",
    "    print(f\"BLEU-1 Score: {avg_bleu1}\")\n",
    "    print(f\"BLEU-4 Score: {avg_bleu4}\")\n",
    "    print(f\"ROUGE-1 Score: {avg_rouge1}\")\n",
    "    print(f\"ROUGE-2 Score: {avg_rouge2}\")\n",
    "    print(f\"ROUGE-L Score: {avg_rougeL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076431d-d0ab-4134-bff3-2e9d6666fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Tokenize the sentences (split by space for simplicity, but consider using more sophisticated tokenization if needed)\n",
    "tokenized_references = [ref.split() for ref in references]\n",
    "tokenized_hypotheses = [hyp.split() for hyp in predictions]\n",
    "# Calculate METEOR scores for each reference-hypothesis pair\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(tokenized_references, tokenized_hypotheses)]\n",
    "\n",
    "# Average the scores (if multiple pairs are present)\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "# Print the results\n",
    "print(f\"METEOR Score: {avg_meteor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
