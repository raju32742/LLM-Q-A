{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6415aa3-2e0e-44d1-a6b8-48c0a68a7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset,Dataset, DatasetDict\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a5d5a-89ef-4fb6-a5ae-f577fef53498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Replace 'your_api_key' with your actual API key\n",
    "wandb.login(key=\"42ec63b91e907bed87b6dc91680e063c2c5cbe27\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6745b-b19b-46f8-a879-9d9601506a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('MedQuAD.csv')  # Replace with your dataset path\n",
    "df = dataset\n",
    "dataset = dataset.drop('qtype', axis=1)\n",
    "dataset = dataset.rename(columns={'Question': 'question', 'Answer': 'answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36166bc0-7b95-43c1-8639-80a03116c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Check the unique values in the 'qtype' column\n",
    "unique_qtypes = df['qtype'].unique()\n",
    "\n",
    "# Display the distribution of question types\n",
    "qtype_distribution = df['qtype'].value_counts()\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "qtype_distribution.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Question Types')\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Display the unique question types\n",
    "print(\"Unique Question Types:\", unique_qtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf4b017-f88e-49eb-834a-be814820f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Answer_Length_Words'] = df['Answer'].str.split().apply(len)\n",
    "# Visualize the distribution of answer lengths\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(df['Answer_Length_Words'], bins=100, color='salmon', edgecolor='black')\n",
    "plt.title('Answer Length Distribution (Words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793885b-77a5-41b1-8404-2d77e121639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# Download the necessary NLTK datasets\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = dataset\n",
    "# Lowercasing\n",
    "# Lowercasing\n",
    "df['question'] = df['question'].str.lower()\n",
    "df['answer'] = df['answer'].str.lower()\n",
    "# Remove punctuation\n",
    "df['question'] = df['question'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df['answer'] = df['answer'].str.translate(str.maketrans('', '', string.punctuation))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c3e36-6df3-4587-a0ad-f01b3212d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of question words to retain\n",
    "question_words = {'who', 'what', 'where', 'when', 'why', 'how', 'is', 'are'}\n",
    "\n",
    "# Define stopwords excluding question words\n",
    "stop_words = set(stopwords.words('english')) - question_words\n",
    "\n",
    "# Remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['question'] = df['question'].apply(remove_stopwords)\n",
    "df['answer'] = df['answer'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf96a9-7f9b-49ac-9c91-ebca50a83834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "# Lemmatization\n",
    "# Initialize stemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()  # Alternative: more aggressive stemming\n",
    "\n",
    "# Function to stem text\n",
    "def stem_text(text, stemmer):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "# Apply stemming\n",
    "df['question'] = df['question'].apply(lambda x: stem_text(x, porter))\n",
    "df['answer'] = df['answer'].apply(lambda x: stem_text(x, porter))\n",
    "\n",
    "# Display processed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2ad3d-0bff-4cb0-8839-ebc56d190e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = df\n",
    "df_full_train, df_test = train_test_split(dataset, test_size=0.2, random_state=56)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538c4d7-1126-42a2-bd62-381836b5eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39637a60-c7b7-4c1c-9337-02c161cf7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a31edd-3b90-445e-a2a2-b5ead5b8c514",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7509e9f3-be53-45d1-8c1c-14124d6965d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BloomTokenizerFast, BloomForCausalLM, Trainer, TrainingArguments\n",
    "# Load the tokenizer and model\n",
    "model_id = \"bigscience/bloom-560m\"\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(model_id)\n",
    "model = BloomForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee94d1-6fc9-4cb4-992e-c5b3b479beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF = 300\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples[\"question\"], truncation=True, padding=\"max_length\", max_length=CUTOFF)\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_datasets = health_dataset_dict.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41748eca-a166-4939-8aac-cbf99860a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = \"MEdQuAD/results/bloom\"  # Replace with a directory where you have write permissions\n",
    "  # Replace with a directory where you have write permissions\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e383c-fc00-452f-a017-13f2d1c57f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490981a-7ab1-4c5e-a42c-a156cf8747b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom compute_loss function\n",
    "import torch.nn.functional as F  # Make sure this is imported for the loss calculation\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('labels').to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        \n",
    "        # Shift logits and labels for causal language modeling\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        # Flatten the tokens to calculate loss\n",
    "        loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa0ea5-aea2-44a9-aa7b-972c2b17bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.001,\n",
    "    logging_dir=os.path.join(output_dir, 'logs'),  # Set directory for logs\n",
    "    logging_strategy=\"steps\",  # Log at each step\n",
    "    logging_steps=10,          # Number of steps between logging\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.001,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=50,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the custom loss computation\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458eb89c-880a-499c-a53a-c0c0acd5fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65185fb5-c6c4-4424-abfe-d85da87aa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae36061-ab8d-4f6f-a21a-76e7926f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "trainer.save_model(\"/kaggle/working/results/bloom/trained_model\")  # Change this to your desired directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277bada-7ac2-432f-8cb2-934ea99bfbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Function to generate predictions\n",
    "def generate_predictions(dataset, model, tokenizer, device, batch_size=16):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Tokenize the inputs\n",
    "        inputs = tokenizer(batch['question'], return_tensors='pt', padding=True, truncation=True, max_length=CUTOFF)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "        attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "        # Generate predictions without updating the model parameters\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=200)\n",
    "        \n",
    "        # Decode the generated tokens into strings\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        \n",
    "        # Extend the predictions and references lists\n",
    "        refs = batch['answer']  # Assumes 'answer' is already tokenized or processed as needed\n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "\n",
    "    return predictions, references\n",
    "\n",
    "# Generate predictions for the test dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predictions, references = generate_predictions(tokenized_datasets['test'], model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c9ee8c-bac4-44ec-833a-0a0626399023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Initialize metrics\n",
    "if len(predictions) == 0 or len(references) == 0:\n",
    "    print(\"No predictions or references to evaluate.\")\n",
    "else:\n",
    "    # Initialize metrics\n",
    "    smooth = SmoothingFunction().method4\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate metrics\n",
    "    bleu1_scores = []\n",
    "    bleu4_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        # BLEU-1 and BLEU-4\n",
    "        bleu1 = sentence_bleu([ref.split()], pred.split(), weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "        bleu4 = sentence_bleu([ref.split()], pred.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
    "        bleu1_scores.append(bleu1)\n",
    "        bleu4_scores.append(bleu4)\n",
    "\n",
    "        # ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "        rouge_scores = rouge.score(ref, pred)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "        \n",
    "    # Check if there are any scores to average\n",
    "    if len(bleu1_scores) == 0:\n",
    "        avg_bleu1 = avg_bleu4 = avg_rouge1 = avg_rouge2 = avg_rougeL = 0\n",
    "    else:\n",
    "        # Average the scores\n",
    "        avg_bleu1 = sum(bleu1_scores) / len(bleu1_scores)\n",
    "        avg_bleu4 = sum(bleu4_scores) / len(bleu4_scores)\n",
    "        avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "        avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "        avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "    # Print the results\n",
    "    print(f\"BLEU-1 Score: {avg_bleu1}\")\n",
    "    print(f\"BLEU-4 Score: {avg_bleu4}\")\n",
    "    print(f\"ROUGE-1 Score: {avg_rouge1}\")\n",
    "    print(f\"ROUGE-2 Score: {avg_rouge2}\")\n",
    "    print(f\"ROUGE-L Score: {avg_rougeL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459fac8-5322-4302-bdf5-1b79e73d341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "# Tokenize the sentences (split by space for simplicity, but consider using more sophisticated tokenization if needed)\n",
    "tokenized_references = [ref.split() for ref in references]\n",
    "tokenized_hypotheses = [hyp.split() for hyp in predictions]\n",
    "# Calculate METEOR scores for each reference-hypothesis pair\n",
    "meteor_scores = [meteor_score([ref], hyp) for ref, hyp in zip(tokenized_references, tokenized_hypotheses)]\n",
    "\n",
    "# Average the scores (if multiple pairs are present)\n",
    "avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "# Print the results\n",
    "print(f\"METEOR Score: {avg_meteor}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
